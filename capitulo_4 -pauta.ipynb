{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contenidos\n",
    "\n",
    "1. [Introducción](#intro)  \n",
    "2. [Regresión](#pp)  \n",
    "3. [Clasificación](#pdSns)  \n",
    "4. [Clustering](#conclu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introducción\n",
    "\n",
    "\n",
    "\n",
    "**¿Qué es el Aprendizaje de Máquinas?**\n",
    "\n",
    "Instalar Plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regresión\n",
    "\n",
    "El problema de regresión consiste en encontrar una relación entre dos variables: entrada y salida, estímulo y respuesta, instancia y etiqueta, etc. El caso más simple, y que sirve de ilustración para modelos más expresivos es el de regresión lineal explorada en la tarea 1. En este módulo, se explora una manera de enriquecer la hipótesis de linealidad en los datos para obtener modelos más expresivos.\n",
    "\n",
    "**Ejercicios**\n",
    "\n",
    "* Cargue los datos presentes en `datos/C4` con el nombre de `aeropuerto.txt`. (Hint:pd.read_fwf)\n",
    "\n",
    "* Separe los datos de manera temporal, para ello almacene el 75% de los datos iniciales en la variable `data_train` y el 25% restante en la variable `data_val`. \n",
    "\n",
    "Para la segunda pregunta, es posible hacer la selección de registros al azar usando las funcionalidades de Pandas, sin embargo, la manera **directa** de hacerlo es usando la librería `scikit-learn`, que corresponde al estándar de uso en la comunidad de machine learning (símil de Numpy en cuando a manejo de arreglos), en este capitulo y en particular en esta sub-sección, se estudian de manera práctica las funcionalidades que esta librería ofrece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T01:15:44.827428Z",
     "start_time": "2018-08-14T01:15:44.633161Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn as sk # importación de la libreria (global)\n",
    "\n",
    "# Importación de la función especifica de \"split\"\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso anterior, la opción `shuffle` permite indicar si se desea particionar los datos de manera aleatoria o secuencial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T03:41:34.868003Z",
     "start_time": "2018-08-14T03:41:34.856709Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_fwf('datos/C4/aeropuerto.txt', header = None, index_col=0)\n",
    "data.columns    = ['Pasajeros']\n",
    "data.index.name = 'Mes'\n",
    "\n",
    "data_train, data_val = train_test_split(data,test_size=0.25, random_state=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez cargados y separados los datos, es necesario visualizarlos:\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "* Grafíque los datos, de manera que los datos de entrenamiento y validación posean colores distintos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T03:41:35.613036Z",
     "start_time": "2018-08-14T03:41:35.411888Z"
    }
   },
   "outputs": [],
   "source": [
    "fig1 = plt.subplots()\n",
    "plt.title('Pasajeros medidos por Mes')\n",
    "\n",
    "plt.scatter(x = data_train.index,y = data_train.Pasajeros)\n",
    "plt.scatter(x = data_val.index,y = data_val.Pasajeros,color ='r')\n",
    "\n",
    "plt.xlabel('Mes'); plt.ylabel('Pasajeros')\n",
    "plt.legend(['Train','Validacion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ajustar una regresión lineal en los datos se utiliza la función `linear_model` del módulo `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T03:41:35.774941Z",
     "start_time": "2018-08-14T03:41:35.765840Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Se inicializa el modelo\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "np_f = lambda x: np.array(x).reshape([-1,1])\n",
    "\n",
    "# Se entrena el modelo\n",
    "x_train = np_f(data_train.index)\n",
    "y_train = np_f(data_train.Pasajeros)\n",
    "\n",
    "regr.fit(x_train,y_train)\n",
    "regr.score(x_train,y_train)\n",
    "\n",
    "# Puntos de vlaidación\n",
    "x_val = np_f(data_val.index)\n",
    "y_val = np_f(data_val.Pasajeros)\n",
    "\n",
    "# Se predice con el modelo\n",
    "y_pred = regr.predict(x_val)\n",
    "\n",
    "print(f'Error cuadrático medio es {sk.metrics.mean_squared_error(y_pred,y_val):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T01:36:54.687205Z",
     "start_time": "2018-08-14T01:36:54.680964Z"
    }
   },
   "source": [
    "Se visualizan los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T03:41:36.322440Z",
     "start_time": "2018-08-14T03:41:36.115507Z"
    }
   },
   "outputs": [],
   "source": [
    "ax, fig = plt.subplots()\n",
    "\n",
    "plt.scatter(x_val,y_val,c='r')\n",
    "plt.plot(x_val,y_pred, 'k--', label='regresión')\n",
    "\n",
    "fig.set_title('Predicción en Validación')\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Ejercicio**\n",
    " \n",
    " * Grafíque la línea de regresión en todo el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T03:41:36.752913Z",
     "start_time": "2018-08-14T03:41:36.544746Z"
    }
   },
   "outputs": [],
   "source": [
    "ax1, fig1 = plt.subplots()\n",
    "x = np_f(data.index)\n",
    "\n",
    "fig1.set_title('Predicción en el dataset')\n",
    "\n",
    "y_pred = regr.predict(x)\n",
    "\n",
    "plt.scatter(x = data_train.index,y = data_train.Pasajeros, label='Train')\n",
    "plt.scatter(x = data_val.index,y = data_val.Pasajeros, color ='r', label='Validación')\n",
    "\n",
    "plt.plot(data.index,y_pred, 'k--', label='regresión')\n",
    "\n",
    "fig1.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como es posible observar, la regresión lineal descubre en cierto grado la *tendencia* de los datos a crecer, sin embargo, si se deseara predecir el comportamiento futuro de la serie de datos, la regresión lineal no sería capaz de proporcionar mayor certeza. Una manera de enriquecer este proceso es transformando los datos u *obteniendo características* de estos. Esta idea da lugar al concepto de regresión no lineal y consiste en modificar los datos iniciales a través de una transformación para luego hacer regresión sobre los \"nuevos\" datos o \"características\" obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "* Utilice `PolynomialFeatures` del módulo `sklearn.preprocessing` para transformar todos los datos de entrada (Mes) a para grados del 1 al 5.\n",
    "\n",
    "* Use todos datos (transformados) para calcular una nueva regresión lineal y compare sus rendimientos. (`sk.metrics.mean_squared_error`)\n",
    "\n",
    "* ¿Qué ocurre cuando se seleccionan los conjuntos de entrenamiento y validación al azar?\n",
    "\n",
    "* Grafíque los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T03:41:38.096134Z",
     "start_time": "2018-08-14T03:41:37.824914Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "mods = []\n",
    "for g in np.arange(1,6):\n",
    "    \n",
    "    poly = PolynomialFeatures(degree=g)\n",
    "    \n",
    "    X = poly.fit_transform(x_train)\n",
    "    x_val_poly = poly.fit_transform(x_val)\n",
    "    \n",
    "    poly_regr = linear_model.LinearRegression()\n",
    "    poly_regr.fit(X, y_train)\n",
    "    \n",
    "    y_pred_poly = poly_regr.predict(x_val_poly)\n",
    "    mods.append(poly_regr)\n",
    "    \n",
    "    print(f'Error cuadrático porlinomio grado {g} es {sk.metrics.mean_squared_error(y_pred_poly,y_val)}')\n",
    "    \n",
    "# Grado Seleccionado\n",
    "g=2\n",
    "\n",
    "ax2, fig2 = plt.subplots()\n",
    "fig2.set_title('Predicción en el dataset con g={}'.format(g))\n",
    "\n",
    "poly = PolynomialFeatures(degree=g)\n",
    "X = poly.fit_transform(x)\n",
    "poly_regr = mods[g-1]\n",
    "\n",
    "y_pred_poly = poly_regr.predict(X)\n",
    "\n",
    "plt.scatter(x = data_train.index,y = data_train.Pasajeros, label='entrenamiento')\n",
    "plt.scatter(x = data_val.index,y = data_val.Pasajeros, color ='r', label='validación')\n",
    "\n",
    "plt.plot(data.index,y_pred_poly, 'k--', label='regresión')\n",
    "\n",
    "fig2.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se pudo ver, la predicción en la tendencia se acerca más a la intuición al escoger una transformación en los datos. La técnica antes implementada se conoce como regresión polinomial y proviene de la clase de regresión lineal generalizada. A continuación, se desea modelar ciertas características fuera de la tendencia del modelo. \n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "* ¿ Qué componente se aprecia fuera de la tendencia ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para modelar las componentes periódicas del problema anterior, se hace una aproximación de los \"residuos\" que ocurren al predecir con el modelo, es decir, se resta a los datos la predicción del modelo encontrado y se aborda el dataset obtenido como un nuevo problema de regresión. Este proceso se puede efectuar las veces que sean necesarias hasta obtener resultados razonables. \n",
    "\n",
    "Para implementar lo anterior:\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "* Genere un nuevo dataset consistente en los residuos del dataset inicial, obtenidos con el modelo de regresión polinomial de grado 2.\n",
    "\n",
    "* Grafíque los residuos tanto para entrenamiento como para validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T03:42:22.302118Z",
     "start_time": "2018-08-14T03:42:22.104970Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_poly = poly_regr.predict(X)\n",
    "\n",
    "x_train_poly = poly.fit_transform(x_train)\n",
    "x_val_poly   = poly.fit_transform(x_val)\n",
    "\n",
    "y_res_train = y_train - poly_regr.predict(x_train_poly)\n",
    "y_res_val = y_val - poly_regr.predict(x_val_poly)\n",
    "\n",
    "ax3, fig3 = plt.subplots()\n",
    "fig3.set_title('Residuos')\n",
    "\n",
    "poly = PolynomialFeatures(degree=g)\n",
    "X = poly.fit_transform(x)\n",
    "poly_regr = mods[g-1]\n",
    "\n",
    "y_pred_poly = poly_regr.predict(X)\n",
    "\n",
    "plt.scatter(x = data_train.index, y = y_res_train, label='Train')\n",
    "plt.scatter(x = data_val.index,y = y_res_val, color ='r', label='Validación')\n",
    "fig3.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizará la siguiente transformación en los datos para modelar las componentes armónicas de los residuos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T03:43:10.264893Z",
     "start_time": "2018-08-14T03:43:10.262136Z"
    }
   },
   "outputs": [],
   "source": [
    "sin_comp  = lambda x,theta: theta[0]*np.sin(theta[1]*x +theta[2])*np.exp(theta[3]*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta transformación no existe un método en `sklearn` por lo que habrá que aprender los parámetros por medio de optimización, para ello se utiliza la librería `scipy` y se define la función a minimizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T03:46:06.463851Z",
     "start_time": "2018-08-14T03:46:06.460067Z"
    }
   },
   "outputs": [],
   "source": [
    "J = lambda theta : np.mean((y_res_train-sin_comp(x_train,theta))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define un punto inicial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T03:46:08.515388Z",
     "start_time": "2018-08-14T03:46:08.454134Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "theta_0 = np.array([1, 0.3, 1.6 , 0.01])\n",
    "\n",
    "h = minimize(fun = J, x0 = theta_0, method='BFGS')\n",
    "\n",
    "theta = h.x\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y se calculan las nuevas predicciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T03:46:10.252584Z",
     "start_time": "2018-08-14T03:46:10.249981Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_sin = sin_comp(x_train,theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "* Grafíque la aproximación a los residuos.\n",
    "* Obtenga la función que aproxima todo el dataset y compárela con la estimación de la tendencia que se hizo inicialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T03:49:26.387334Z",
     "start_time": "2018-08-14T03:49:26.207900Z"
    }
   },
   "outputs": [],
   "source": [
    "ax4, fig4 = plt.subplots()\n",
    "fig4.set_title('Residuos aproximados')\n",
    "\n",
    "plt.scatter(x = data_train.index, y = y_res_train, label='Train')\n",
    "plt.scatter(x = data_val.index,y = y_res_val, color ='r', label='Validación')\n",
    "plt.plot(y_pred_sin)\n",
    "fig4.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T03:52:14.210155Z",
     "start_time": "2018-08-14T03:52:14.012266Z"
    }
   },
   "outputs": [],
   "source": [
    "f_pol_sin = lambda x: poly_regr.predict(poly.fit_transform(x)) + sin_comp(x,theta=theta)\n",
    "y_pred_final = f_pol_sin(x)\n",
    "\n",
    "plt.scatter(x = data_train.index, y = y_train, label='Train')\n",
    "plt.scatter(x = data_val.index,y = y_val, color ='r', label='Validación')\n",
    "plt.plot(y_pred_final, 'k--', label='Predicción')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método de regresión jerárquica por residuos ofrece gran flexibilidad a la hora de enfrentar tareas de regresión. Como se pudo ver en este módulo, es posible incluso implementar transformaciones de datos no incluidas en los paquetes tradicionales. Se recomienda explorar los otros métodos de regresión como son los procesos gaussianos y support vector machines presentes en `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Clasificación \n",
    "\n",
    "El problema de clasificación, corresponde al igual que el de regresión, a encontrar una relación entre entradas y salidas de datos, la diferencia entre ambos problema radica en que el problema de clasificación consiste en asignar valores categóricos o discretos mientras que en regresión estos son continuos. A continuación se estudian métodos de _machine learning_ aplicados en este contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T07:43:12.041108Z",
     "start_time": "2018-08-14T07:43:12.038225Z"
    }
   },
   "outputs": [],
   "source": [
    "# Modelos de clasificación estudiados\n",
    "from sklearn.linear_model import LogisticRegression #regresión logística\n",
    "from sklearn.svm import SVC, LinearSVC #support vector machines\n",
    "from sklearn.ensemble import RandomForestClassifier #random forests\n",
    "from sklearn.neighbors import KNeighborsClassifier #K-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "* Cargue los datos presentes en `datos/C4` con el nombre de `titanic.csv`. (Hint:pd.read_csv)\n",
    "* Separe los datos de manera aleatoria, para ello almacene el 75% de los datos iniciales en la variable train_df y el 25% restante en la variable val_df.\n",
    "\n",
    "* Describa los datos importados. ¿Qué columnas contienen valores nulos o mal ingresados?\n",
    "\n",
    "Observación: `desribe(include=['O'])`permite estudiar las variables categóricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T07:43:13.073182Z",
     "start_time": "2018-08-14T07:43:13.062897Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('datos/C4/titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T07:43:19.914473Z",
     "start_time": "2018-08-14T07:43:19.897129Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df , val_df = train_test_split(df,test_size=0.25, random_state=1, shuffle=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T07:43:20.464955Z",
     "start_time": "2018-08-14T07:43:20.460518Z"
    }
   },
   "outputs": [],
   "source": [
    "val_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T07:43:21.221121Z",
     "start_time": "2018-08-14T07:43:21.212378Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T07:43:22.523105Z",
     "start_time": "2018-08-14T07:43:22.493396Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de los datos\n",
    "\n",
    "**Datos Faltantes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T07:43:25.258142Z",
     "start_time": "2018-08-14T07:43:25.243473Z"
    }
   },
   "outputs": [],
   "source": [
    "valores_faltantes = pd.concat([train_df.isnull().sum(), val_df.isnull().sum()],\n",
    "                              axis=1, sort=True,keys=['Train', 'Val']) \n",
    "valores_faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agrupaciones de interés**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T07:43:26.644331Z",
     "start_time": "2018-08-14T07:43:26.635062Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T07:43:27.240488Z",
     "start_time": "2018-08-14T07:43:27.228123Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de datos\n",
    "\n",
    "**Age**\n",
    "\n",
    "Se agregan edades aleatorias dentro del promedio existente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T07:43:31.245085Z",
     "start_time": "2018-08-14T07:43:29.258772Z"
    }
   },
   "outputs": [],
   "source": [
    "new_ages_train = np.random.randint(train_df[\"Age\"].mean() - train_df[\"Age\"].std(),\n",
    "                                   train_df[\"Age\"].mean() + train_df[\"Age\"].std(),\n",
    "                                    size = train_df[\"Age\"].isnull().sum())\n",
    "\n",
    "new_ages_val  = np.random.randint(val_df[\"Age\"].mean() - val_df[\"Age\"].std(),\n",
    "                                  val_df[\"Age\"].mean() + val_df[\"Age\"].std(),\n",
    "                                  size = val_df[\"Age\"].isnull().sum())\n",
    "\n",
    "train_df['Age'][train_df[\"Age\"].isnull()] = new_ages_train\n",
    "val_df[\"Age\"][val_df[\"Age\"].isnull()]   = new_ages_val\n",
    "\n",
    "train_df['Age'] = train_df['Age'].astype(int)\n",
    "val_df['Age']  = val_df['Age'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embarked y Port**\n",
    "\n",
    "Se agrega la categoria 'S' y se mapea con índices discretos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T07:43:34.238523Z",
     "start_time": "2018-08-14T07:43:33.235102Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df[\"Embarked\"].fillna('S', inplace=True)\n",
    "val_df[\"Embarked\"].fillna('S', inplace=True)\n",
    "\n",
    "train_df['Port'] = train_df['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "val_df['Port'] = val_df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)\n",
    "\n",
    "del train_df['Embarked']\n",
    "del val_df['Embarked']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cabin**\n",
    "\n",
    "Se elimina por poseer demasiados valores faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T07:43:35.646749Z",
     "start_time": "2018-08-14T07:43:35.642795Z"
    }
   },
   "outputs": [],
   "source": [
    "del train_df['Cabin']\n",
    "del val_df['Cabin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name y Ticket**\n",
    "\n",
    "Se eliminan por su poca relevancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T07:43:36.870023Z",
     "start_time": "2018-08-14T07:43:36.861632Z"
    }
   },
   "outputs": [],
   "source": [
    "del train_df['Name']\n",
    "del train_df['Ticket']\n",
    "\n",
    "del val_df['Name']\n",
    "del val_df['Ticket']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T07:32:57.170836Z",
     "start_time": "2018-08-14T07:32:57.167041Z"
    }
   },
   "source": [
    "**Sex**\n",
    "\n",
    "Se traspasa a variable discreta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T07:43:38.253522Z",
     "start_time": "2018-08-14T07:43:38.238841Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.get_dummies(train_df,columns=['Sex'])\n",
    "val_df   = pd.get_dummies(val_df,columns=['Sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T07:44:35.532080Z",
     "start_time": "2018-08-14T07:44:35.523551Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_df.drop(\"Survived\", axis=1)\n",
    "Y_train = train_df[\"Survived\"]\n",
    "\n",
    "X_val  = val_df.drop(\"PassengerId\", axis=1).copy()\n",
    "print('Dimensiones de los datos son:')\n",
    "X_train.shape, Y_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T07:44:50.375621Z",
     "start_time": "2018-08-14T07:44:50.364659Z"
    }
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = logreg.predict(X_val)\n",
    "acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n",
    "print(f'Precisión de regresión logística es {acc_log} (en entrenamiento)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suport Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T07:47:06.786430Z",
     "start_time": "2018-08-14T07:47:06.740629Z"
    }
   },
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = svc.predict(X_val)\n",
    "acc_svc = round(svc.score(X_train, Y_train) * 100, 2)\n",
    "print(f'Precisión de support vector machines es {acc_svc}  (en entrenamiento)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T07:48:14.213711Z",
     "start_time": "2018-08-14T07:48:14.019399Z"
    }
   },
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = random_forest.predict(X_val)\n",
    "random_forest.score(X_train, Y_train)\n",
    "\n",
    "acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n",
    "print(f'Precisión de random forests es {acc_random_forest}  (en entrenamiento)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T07:47:04.133832Z",
     "start_time": "2018-08-14T07:47:04.120963Z"
    }
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = knn.predict(X_val)\n",
    "acc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n",
    "print(f'Precisión de k nearest neighbors es {acc_knn} (en entrenamiento)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T07:49:09.378021Z",
     "start_time": "2018-08-14T07:49:09.364952Z"
    }
   },
   "outputs": [],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n",
    "              'Random Forest'],\n",
    "    'Score': [acc_svc, acc_knn, acc_log, \n",
    "              acc_random_forest]})\n",
    "models.sort_values(by='Score', ascending=False)\n",
    "print('agreguemos validación :)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Clustering\n",
    "## Encuesta de felicidad\n",
    "\n",
    "El proceso de clustering o aglomeración consiste en agrupar una serie de vectores o datos según cierto criterio, éste por lo general se basa en distancia o similitud. \n",
    "\n",
    "Como se vio en el Capítulo 1, se podía definir un algoritmo de aglomeración según la distancia euclidiana, que se generalizó a la distancia $p$. En general, se busca que la métrica de aglomeración en conjunción con el modelo que se implementa, permitan obtener información no trivial sobre los datos que se estudian, así como también obtener representaciones o etiquetas de estos de manera *no supervisada*.\n",
    "\n",
    "Siguiendo el esquema de los módulos anteriores, se observan algunos modelos de clustering:\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "* Cargue los datos presentes en datos/C4 con el nombre de felicidad_2017.csv. (Hint:pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T04:59:59.188184Z",
     "start_time": "2018-08-14T04:59:59.153831Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('datos/C4/felicidad_2017.csv', header=0)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ejecute la siguiente orden.¿Qué patrones pudo observar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T05:07:49.713628Z",
     "start_time": "2018-08-14T05:07:49.678760Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "data = dict(type = 'choropleth', \n",
    "           locations = df['Country'],\n",
    "           locationmode = 'country names',\n",
    "           z = df['Happiness.Rank'], \n",
    "           text = df['Country'],\n",
    "           colorbar = {'title':'Happiness'})\n",
    "\n",
    "layout = dict(title = 'Global Happiness', \n",
    "             geo = dict(showframe = False, \n",
    "                       projection = {'type': 'natural earth'}))\n",
    "choromap3 = go.Figure(data = [data], layout=layout)\n",
    "iplot(choromap3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use seaborn para investigar las correlaciones entre las variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T05:13:38.258888Z",
     "start_time": "2018-08-14T05:13:27.206458Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig=plt.figure()\n",
    "sns.pairplot(df, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según lo anterior, se escogen 3 variables de interés dentro del dataset, estas se visualizan de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T05:31:28.554578Z",
     "start_time": "2018-08-14T05:31:28.347546Z"
    }
   },
   "outputs": [],
   "source": [
    "ax, fig = plt.subplots()\n",
    "\n",
    "fig.set_title('Ranking según índices seleccionados')\n",
    "\n",
    "rank = df['Happiness.Rank']\n",
    "plt.scatter(df['Economy..GDP.per.Capita.'],rank,label='Economy')\n",
    "plt.scatter(df['Generosity'],rank,label='Generosity')\n",
    "plt.scatter(df['Dystopia.Residual'],rank,label='Dys_res')\n",
    "plt.ylabel('happiness ranking')\n",
    "plt.xlabel('valor del índice')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando el módulo de clustering de sklearn, se usarán distintos algoritmos de aglomeración y se observará su efecto en el mapa antes generado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T06:09:44.698165Z",
     "start_time": "2018-08-14T06:09:44.693405Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se obtiene la lista de paises \n",
    "country = df.Country\n",
    "\n",
    "# Se trabaja sobre los datos sin las columnas 'Country' y 'Happiness Rank'\n",
    "data = df.drop(['Happiness.Rank','Country'], axis=1)\n",
    "cols = data.columns \n",
    "\n",
    "# Se estandarizan los datos\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "data = pd.DataFrame(sc.fit_transform(data))\n",
    "data.columns = cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means\n",
    "\n",
    "Este algoritmo ya fue estudiado en el Capítulo 1, ahora se usará la implementación que ofrece `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T06:09:54.916286Z",
     "start_time": "2018-08-14T06:09:54.875412Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster, mixture\n",
    "\n",
    "k = 4\n",
    "\n",
    "kmeans_alg = cluster.KMeans(k)\n",
    "kmeans_res = kmeans_alg.fit_predict(data)\n",
    "\n",
    "df_km = data.copy()\n",
    "\n",
    "df_km['Kmeans']= kmeans_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T06:09:55.462379Z",
     "start_time": "2018-08-14T06:09:55.448985Z"
    }
   },
   "outputs": [],
   "source": [
    "df_km.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "* Grafíque los grupos encontrados para las variables 'Happiness.Score' , 'Dystopia.Residual'.¿Qué cantidad de clusters elige?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T06:09:56.556106Z",
     "start_time": "2018-08-14T06:09:56.402856Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(df_km['Happiness.Score'], df_km['Dystopia.Residual'],  c=kmeans_res)\n",
    "plt.ylabel('happiness score ')\n",
    "plt.xlabel('valor del índice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Agregue al dataframe `df_km` con los valores 'Country' del dataframe `data` y visualice con el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T06:09:57.537658Z",
     "start_time": "2018-08-14T06:09:57.533302Z"
    }
   },
   "outputs": [],
   "source": [
    "df_km['Country'] = country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T06:09:58.296826Z",
     "start_time": "2018-08-14T06:09:58.260611Z"
    }
   },
   "outputs": [],
   "source": [
    "dataPlot = dict(type = 'choropleth', \n",
    "           locations = df_km['Country'],\n",
    "           locationmode = 'country names',\n",
    "           z = df_km['Kmeans'], \n",
    "           text = df_km['Country'],\n",
    "           colorbar = {'title':'Cluster Group'})\n",
    "layout = dict(title = 'Kmeans Clustering', \n",
    "           geo = dict(showframe = False, \n",
    "           projection = {'type': 'natural earth'}))\n",
    "choromap3 = go.Figure(data = [dataPlot], layout=layout)\n",
    "iplot(choromap3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering espectral\n",
    "\n",
    "**Ejercicio** \n",
    "\n",
    "* Aplique la metodología anterior para la función `cluster.SpectralClustering`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T06:14:14.724821Z",
     "start_time": "2018-08-14T06:14:14.650116Z"
    }
   },
   "outputs": [],
   "source": [
    "k=4\n",
    "\n",
    "spectral = cluster.SpectralClustering(n_clusters=k)\n",
    "sp_res= spectral.fit_predict(data)\n",
    "\n",
    "df_sp = data.copy()\n",
    "df_sp['spectral'] = pd.DataFrame(sp_res)\n",
    "df_sp['Country'] = country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T06:14:15.333486Z",
     "start_time": "2018-08-14T06:14:15.148488Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(df_km['Happiness.Score'], df_km['Dystopia.Residual'],  c= sp_res)\n",
    "plt.ylabel('happiness score ')\n",
    "plt.xlabel('valor del índice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T06:14:17.090419Z",
     "start_time": "2018-08-14T06:14:17.049597Z"
    }
   },
   "outputs": [],
   "source": [
    "dataPlot = dict(type = 'choropleth', \n",
    "           locations = df_sp['Country'],\n",
    "           locationmode = 'country names',\n",
    "           z = df_sp['spectral'], \n",
    "           text = df_sp['Country'],\n",
    "           colorbar = {'title':'Cluster Group'})\n",
    "layout = dict(title = 'Spectral Clustering', \n",
    "           geo = dict(showframe = False, \n",
    "           projection = {'type': 'natural earth'}))\n",
    "choromap3 = go.Figure(data = [dataPlot], layout=layout)\n",
    "iplot(choromap3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T06:22:38.198430Z",
     "start_time": "2018-08-14T06:22:38.012198Z"
    }
   },
   "outputs": [],
   "source": [
    "eps = 1.35\n",
    "\n",
    "db     = cluster.DBSCAN(eps)\n",
    "db_res = db.fit_predict(data)\n",
    "\n",
    "df_db = data.copy()\n",
    "df_db['DBSCAN'] = pd.DataFrame(sp_res)\n",
    "df_db['Country'] = country\n",
    "\n",
    "plt.scatter(df_db['Happiness.Score'], df_db['Dystopia.Residual'],  c= db_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T06:23:04.719850Z",
     "start_time": "2018-08-14T06:23:04.679265Z"
    }
   },
   "outputs": [],
   "source": [
    "dataPlot = dict(type = 'choropleth', \n",
    "           locations = df_sp['Country'],\n",
    "           locationmode = 'country names',\n",
    "           z = df_db['DBSCAN'], \n",
    "           text = df_db['Country'],\n",
    "           colorbar = {'title':'Cluster Group'})\n",
    "layout = dict(title = 'DBSCAN', \n",
    "           geo = dict(showframe = False, \n",
    "           projection = {'type': 'natural earth'}))\n",
    "choromap3 = go.Figure(data = [dataPlot], layout=layout)\n",
    "iplot(choromap3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T06:24:34.997450Z",
     "start_time": "2018-08-14T06:24:34.809937Z"
    }
   },
   "outputs": [],
   "source": [
    "comps = 4\n",
    "\n",
    "gmm = mixture.GaussianMixture(n_components=comps,covariance_type='full')\n",
    "\n",
    "gmm.fit(data)\n",
    "gmm_res =gmm.predict(data)\n",
    "\n",
    "\n",
    "df_gmm = data.copy()\n",
    "df_gmm['GMM'] = pd.DataFrame(gmm_res)\n",
    "df_gmm['Country'] = country\n",
    "\n",
    "plt.scatter(df_gmm['Happiness.Score'], df_gmm['Dystopia.Residual'],  c= gmm_res)\n",
    "plt.ylabel('happiness score ')\n",
    "plt.xlabel('valor del índice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T06:25:28.032266Z",
     "start_time": "2018-08-14T06:25:27.995794Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset=pd.concat([data,country],axis=1)\n",
    "dataPlot = dict(type = 'choropleth', \n",
    "           locations = df_gmm['Country'],\n",
    "           locationmode = 'country names',\n",
    "           z = df_gmm['GMM'], \n",
    "           text = dataset['Country'],\n",
    "           colorbar = {'title':'Cluster Group'})\n",
    "layout = dict(title = 'GMM', \n",
    "           geo = dict(showframe = False, \n",
    "           projection = {'type': 'natural earth'}))\n",
    "choromap3 = go.Figure(data = [dataPlot], layout=layout)\n",
    "iplot(choromap3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "303px",
    "left": "1472.45px",
    "right": "20px",
    "top": "134px",
    "width": "230px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
