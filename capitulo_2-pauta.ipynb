{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contendidos\n",
    "\n",
    "1. [Introducción](#intro)  \n",
    "2. [Elementos Básicos de Pandas](#elementos)  \n",
    "  2.1 [Series](#series)  \n",
    "  2.2 [DataFrame](#df)  \n",
    "  2.3 [Funcionalidades esenciales](#fun_es)  \n",
    "3. [Estadistica descriptiva básica](#estaDesc)  \n",
    "4. [Limpieza y preparación de datos](#limPrep)\n",
    "5. [Carga y guardado de datos](#carga)  \n",
    "6. [Manejo de API's](#API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción<a id=\"intro\"></a>\n",
    "\n",
    "El manejo adecuado de datos es (naturalmente) un aspecto fundamental en ciencia de datos. Como se ha visto, `NumPy` permite un manejo básico de datos a través de sus operaciones sobre arreglos. No obstante, las tareas referentes al manejo de datos requieren habitualmente de funcionalidades más especificas. \n",
    "\n",
    "En este contexto nace la librería `Pandas`. Esta se concibió como una extensión de `NumPy` basada en software libre y dirigida específicamente a la manipulación y análisis de datos en Python. \n",
    "\n",
    "`Pandas` provee estructuras y operaciones para el trabajo de tablas numéricas y series de tiempo, es estándar en aplicaciones de ciencia de datos (basadas en Python). Se usa en conjunto con librerías de computación numérica (como `Numpy` y `SciPy`), librerías de visualización (como `matplotlib` y `seaborn`), librerías de analítica (como `statsmodels` y `scikit-learn`), entre otras.\n",
    "\n",
    "El manejo de datos con `Pandas` toma los elementos de `Numpy` en cuanto a computación basada en arreglos y los expande al manejo de datos heterogéneos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elementos básicos de Pandas<a id=\"elementos\"></a>\n",
    "\n",
    "Como convención, `Pandas` se importa de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:17.627834Z",
     "start_time": "2018-07-25T02:10:17.363190Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las estructuras de datos más usadas en esta librería corresponden a *Series* y *DataFrame*\n",
    "\n",
    "## Series<a id=\"series\"></a>\n",
    "Una serie es un objeto cuya estructura consiste en un arreglo unidimensional que contiene una sucesión de valores al cual se asocia un nuevo arreglo con las etiquetas de los datos, este último arreglo se denota como índice o `index`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:18.012576Z",
     "start_time": "2018-07-25T02:10:17.990671Z"
    }
   },
   "outputs": [],
   "source": [
    "serie = pd.Series([1,9,7, -5, 3,10])\n",
    "serie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La operación anterior imprime en pantalla los valores de la serie y a su izquierda su índice correspondiente. Es posible especificar el formato sobre la indexación, el valor por defecto es el anterior y consiste en valores enteros entre `0`  y `N-1`, donde `N` es la cantidad de valores en la serie.\n",
    "\n",
    "Para especificar los índices en cada elemento de la serie, se puede acceder al atributo `index` de esta y modificarlo o especificar la configuración deseada al declarar la serie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:18.323208Z",
     "start_time": "2018-07-25T02:10:18.314359Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cambiar atributo \"index\" una vez definida la serie\n",
    "serie.index = range(1,7)\n",
    "\n",
    "print('serie, indice como rango: \\n')\n",
    "print(serie, '\\n')\n",
    "\n",
    "serie.index = ['a','b','c','d','e','f']\n",
    "print('serie, indice como lista: \\n')\n",
    "print(serie, '\\n')\n",
    "\n",
    "# Definir indice en la declaración de la serie\n",
    "\n",
    "serie_2 = pd.Series([9,7, -5, 3], index=['a1', 'a2', 'a3', 'a1000'])\n",
    "print('serie_2 delcarado con indice como lista: \\n')\n",
    "print(serie_2, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible seleccionar elementos de la serie usando su etiqueta, de manera similar a como se hace en `Numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:18.607171Z",
     "start_time": "2018-07-25T02:10:18.602227Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se seleccionobja el valor de obj2, cuya etiqueta es 'a1000'\n",
    "serie_2['a1000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios**\n",
    "\n",
    "* Cree una serie `obj` con los valores `a`, `c` y `e` de `serie`.\n",
    "* Imprima en pantalla aquellos valores de `obj` que sean mayores que 3.\n",
    "* Cree una serie `obj_3` que contenga todos los múltiplos de 3, menores que 100. \n",
    "* Imprima en pantalla los elementos de `obj_3` que sean múltiplos de 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las series de `Pandas` soportan operaciones *elemento a elemento*, estas abarcan: filtrado booleano (ejercicio anterior), multiplicación escalar y en general, funciones matemáticas como la suma, logaritmo, exponencial, etc... \n",
    "\n",
    "Las operaciones sobre elementos de una serie son completamente compatibles con las funciones de `Numpy` y preservan los índices de los elementos operados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:19.150845Z",
     "start_time": "2018-07-25T02:10:19.107700Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\n Serie 2 duplicada \\n')\n",
    "print(serie_2*2)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print('\\n Serie 2 operada por \"exp()\" de Numpy: \\n')\n",
    "print(np.exp(serie_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los índices permiten trabajar con series de manera similar a los diccionarios de `NumPy` por medio del operador `in`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:19.472926Z",
     "start_time": "2018-07-25T02:10:19.467632Z"
    }
   },
   "outputs": [],
   "source": [
    "['a3' in serie_2, 'a4' in serie_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede además crear una serie de `Pandas` a partir de un diccionario `NumPy`, donde el conjunto de índices corresponderá al conjunto de campos del diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:19.899245Z",
     "start_time": "2018-07-25T02:10:19.887972Z"
    }
   },
   "outputs": [],
   "source": [
    "dic = {'a1': 7, 'a2': 6.5 , 'a3': 5, 'a4':2.1, 'a1000':1 }\n",
    "serie_a = pd.Series(dic)\n",
    "\n",
    "print('\\n serie_a: serie a partir de un diccionario np: \\n')\n",
    "print(serie_a)\n",
    "\n",
    "idx_b = ['a1','a2','b']\n",
    "serie_b = pd.Series(serie_a, index=idx_b)\n",
    "\n",
    "print('\\n serie_b: indice extra, subseleccion serie_a \\n')\n",
    "print(serie_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En `serie_b` se toman los valores `a1` y `a2` de `serie_a` y se agrega una nueva etiqueta `b`, ausente en `dic` y por tanto ausente `serie_a`. La declaración de `serie_b` toma por tanto los valores existentes y correspondientes a cada índice, asignado en valor `NaN` (not a number) a aquellos índices no existentes en el diccionario inicial (`serie_2`). Se aprecia además que la declaración `pd.Series` no diferencia entre un diccionario `NumPy` y una serie de `Pandas`.\n",
    "\n",
    "El término `b` en `serie_b` es un dato ausente (missing data, NA), las funciones `isnull` and `notnull` manejan este tipo de entradas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:20.275683Z",
     "start_time": "2018-07-25T02:10:20.271264Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.isnull(serie_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:20.465858Z",
     "start_time": "2018-07-25T02:10:20.456430Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.notnull(serie_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas funciones son métodos de los objetos `Series` y se puede por lo tanto acceder directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:20.844766Z",
     "start_time": "2018-07-25T02:10:20.837537Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Método isnull: \\n')\n",
    "print(serie_b.isnull())\n",
    "print('\\n')\n",
    "print('Método notnull: \\n')\n",
    "print(serie_b.notnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tipo de objetos presenta además la posibilidad de realizar operaciones aritméticas, preservando el alineamiento de índices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:21.202969Z",
     "start_time": "2018-07-25T02:10:21.195303Z"
    }
   },
   "outputs": [],
   "source": [
    "# Suma de series, NaN se comporta como el \"infinito matematico\" \n",
    "serie_b + serie_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "por último, las series de `Pandas` permite el etiquetado de variables, lo cual acerca más su manejo al usado en bases de datos y lo enriquece en comparación a `Numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:21.562588Z",
     "start_time": "2018-07-25T02:10:21.558656Z"
    }
   },
   "outputs": [],
   "source": [
    "serie_a.name = 'notas'\n",
    "serie_a.index.name = 'alumnos'\n",
    "print(serie_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## DataFrame<a id=\"df\"></a>\n",
    "\n",
    "Una serie representa un arreglo unidimensional enriquecido por índices y manejo de valores faltantes. Un DataFrame por su parte, representa una tabla rectangular, donde los datos están contenidos\n",
    "en una estructura ordenada y basada en columnas, las cuales pueden ser de distintos tipos (`int`, `str`, `bool`).\n",
    "\n",
    "Un `DataFrame` posee índices para sus columnas y sus filas. Al igual que la series, se puede observar como un diccionario de `NumPy`  donde cada campo es una serie, cada una de las cuales, comparten el mismo índice. Si bien esta noción describe un objeto 2-dimensional, es posible obtener representaciones dimensionalmente superiores mediante una indexación adecuada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen varias formas de construir un `DataFrame`, una manera conveniente, consiste en definir un diccionario de listas, todas de igual longitud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:22.148217Z",
     "start_time": "2018-07-25T02:10:22.132061Z"
    }
   },
   "outputs": [],
   "source": [
    "data = {'ciudad': ['Temuco', 'Temuco', 'Temuco', 'Iquique', 'Iquique', 'Iquique'],\n",
    "'year': [2000, 2001, 2002, 2001, 2002, 2003],\n",
    "'pob': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}\n",
    "\n",
    "frame = pd.DataFrame(data)\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pandas` detecta el entorno `Jupyter` e imprime en pantalla los datos usando un formato HTML. (Observación: no se usó la función `print`)\n",
    "\n",
    "En DataFrames consistentes de muchas observaciones, es posible mostrar una cantidad reducida de elementos usando el método `head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:22.508868Z",
     "start_time": "2018-07-25T02:10:22.501533Z"
    }
   },
   "outputs": [],
   "source": [
    "# Muestra las primeras 3 observaciones\n",
    "frame.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede manipular el orden de las columnas, en el caso de agregar una columna no existente, al igual que con series, se incluirá `NaN` indicando ausencia de datos, de igual manera, se pueden editar los índices correspondiente a cada observación del DataFrame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:22.914350Z",
     "start_time": "2018-07-25T02:10:22.896200Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = ['year','ciudad','pob','pais']\n",
    "idx  = ['uno', 'dos', 'tres', 'cuatro', 'cinco', 'seis']\n",
    "\n",
    "frame = pd.DataFrame(data, columns= cols, index= idx)\n",
    "\n",
    "print('Columnas de frame: ',frame.columns)\n",
    "print('Indices de frame: ',frame.index)\n",
    "\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al seleccionar una columna del DataFrame, se obtiene una serie. Tal serie se puede obtener usando la notación de `Numpy` o como un atributo del objeto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:23.297331Z",
     "start_time": "2018-07-25T02:10:23.286862Z"
    }
   },
   "outputs": [],
   "source": [
    "# Acceder a la columna \"pob\" de frame usando notación de Numpy\n",
    "print(frame['pob'], '\\n')\n",
    "\n",
    "# Acceder a la columna \"pob\" de frame como un atributo del objeto\n",
    "print(frame.pob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análogamente, se pueden obtener filas del DataFrame usando sus índices a través del método `loc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:23.688979Z",
     "start_time": "2018-07-25T02:10:23.683428Z"
    }
   },
   "outputs": [],
   "source": [
    "frame.loc['uno']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De bastante interés para el manejo de bases de datos, es la asignación de valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:24.089308Z",
     "start_time": "2018-07-25T02:10:24.076041Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se asigna un valor numerico distinto a cada valor de la variable pais\n",
    "frame['pais'] =  range(1,7)\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:24.287590Z",
     "start_time": "2018-07-25T02:10:24.272620Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se asigna el valor \"Chile\" a cada valor de la columna \"pais\"\n",
    "frame.pais = 'Chile'\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la ayuda de series, es posible asignar valores para índices específicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:24.655698Z",
     "start_time": "2018-07-25T02:10:24.646655Z"
    }
   },
   "outputs": [],
   "source": [
    "val = pd.Series(['cl','cl','cl'], index=['uno','tres','seis'])\n",
    "frame['pais'] = val\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al asignar valores de esta forma, la variable `val` no tiene un valor definido para los índices `dos`,`cuatro` y `cinco`, por lo que los valores de estos índices pasa a ser `NaN` incluso cuando antes eran `Chile`. \n",
    "\n",
    "Es posible agregar columnas a través de operaciones lógicas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:25.055879Z",
     "start_time": "2018-07-25T02:10:25.043104Z"
    }
   },
   "outputs": [],
   "source": [
    "frame['sur'] = frame.ciudad == 'Temuco'\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Si la se quisiera a agregar la columna `norte` usando la sintaxis `frame.norte = frame.ciudad == 'Iquique'` no seria posible, eso pues Python intentará acceder al atributo `frame.norte` que es inexistente aún. Por otro lado, la notación `frame['norte'] = frame.ciudad == 'Iquique'` construye el atributo `norte` que podrá ser accedido por `frame.norte`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:25.445194Z",
     "start_time": "2018-07-25T02:10:25.438208Z"
    }
   },
   "outputs": [],
   "source": [
    "frame.norte = frame.ciudad == 'Iquique'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:25.639859Z",
     "start_time": "2018-07-25T02:10:25.627180Z"
    }
   },
   "outputs": [],
   "source": [
    "frame['norte'] = frame.ciudad == 'Iquique'\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otra parte, es posible eliminar una columna usando el comando `del`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:26.755487Z",
     "start_time": "2018-07-25T02:10:26.742367Z"
    }
   },
   "outputs": [],
   "source": [
    "del frame['pais']\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, es posible renombrar los índices (filas, columnas) de un DataFrame;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:27.271946Z",
     "start_time": "2018-07-25T02:10:27.258408Z"
    }
   },
   "outputs": [],
   "source": [
    "frame.index.name='registro'\n",
    "frame.columns.name='datos'\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "* Cargue los datos de la clase pasada usando `np.load`. Unifique los datos importados en un DataFrame cuyas columnas sean `SCI` (Statistical Capacity Index), `HDI` (Human development Index), `Country` y `Reg` (region).\n",
    "\n",
    "* Muestre en pantalla los registros del DataFrame anterior para Chile, Perú, Argentina y Bolivia. (Hint: use el método `isin` en una lista para la variable `Country`) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcionalidades esenciales<a id=\"fun_es\"></a>\n",
    "\n",
    "A continuación, se trabaja con los aspecto fundamentales referentes al manejo de Series y DataFrames.\n",
    "\n",
    "### Reindexación\n",
    "\n",
    "Los objetos de `Pandas` contienen el método `reindex`, este sirve para crear un nuevo objeto a partir de uno inicial, al cual se **permutan** y agregan índices. Al igual que la declaración de índices nuevos en DataFrames y en Series, si se añade un índice nuevo a través de `reindexing` este incluirá valores faltantes `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:28.294665Z",
     "start_time": "2018-07-25T02:10:28.289261Z"
    }
   },
   "outputs": [],
   "source": [
    "print(serie)\n",
    "serie_3 = serie.reindex(['b','a','z'])\n",
    "\n",
    "print('\\n Serie reindexada: \\n')\n",
    "print(serie_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando el orden de indexación, `Pandas` puede completar valores faltantes usando el método `ffill` de llenado hacia adelante (forward):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:28.873963Z",
     "start_time": "2018-07-25T02:10:28.863606Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Sin llenado:','\\n')\n",
    "serie_4 = serie.reindex(['b','z','h','a'])\n",
    "print(serie_4, '\\n')\n",
    "\n",
    "print('Con llenado:','\\n')\n",
    "serie_4 = serie.reindex(['b','z','h','a'], method='ffill')\n",
    "print(serie_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminar valores por eje<a id=\"link\"></a>\n",
    "\n",
    "Los ejes en `Pandas` se denotan por `axis` donde `axis = 0` representa los índices (o filas), mientas que `axis = 1` representa las columnas. \n",
    "\n",
    "Para eliminar valores de un DataFrame o Serie tanto en filas como en columnas, existe el método `drop`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:19.033342Z",
     "start_time": "2018-07-25T02:15:19.017257Z"
    }
   },
   "outputs": [],
   "source": [
    "data = np.load('../cepal_estudiantes/datos/C1/SCI_HDI.npy')\n",
    "CR = np.load('../cepal_estudiantes/datos/C1/SCI_HDI_CR.npy')\n",
    "data_CR = np.hstack((data,CR))\n",
    "\n",
    "df = pd.DataFrame(data_CR, columns=['SCI','HDI','Country','Reg'])\n",
    "\n",
    "DF = df[df.Country.isin(['Chile','Peru','Argentina','Bolivia'])]\n",
    "DF.sort_values('HDI',axis=0, ascending=False)\n",
    "\n",
    "# Se indexa el DataFrame anterior por los paises\n",
    "DF.index = DF.Country\n",
    "\n",
    "# Se elimina la columna Country (axis = 1)\n",
    "df_drop_cl = DF.drop('Country',axis=1)\n",
    "print('\\n',df_drop_cl,'\\n')\n",
    "\n",
    "# Se elimina la fila cuyo indice es 'Chile' (axis = 0, por defecto)\n",
    "df_drop_cl = df_drop_cl.drop('Chile')\n",
    "print(df_drop_cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "* Elimine la columna `Reg` y renombre las columnas de `df_drop_cl` como `Idx`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexación, selección y filtrado\n",
    "\n",
    "Como se vio anteriormente, las series en `Pandas` trabajan de manera similar a los arreglos de `Numpy`, los DataFrame comparten la misma propiedad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:20.229576Z",
     "start_time": "2018-07-25T02:15:20.219185Z"
    }
   },
   "outputs": [],
   "source": [
    "# Muestra todas las filas menos la última\n",
    "df_drop_cl[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:20.664945Z",
     "start_time": "2018-07-25T02:15:20.656523Z"
    }
   },
   "outputs": [],
   "source": [
    "# Selecion por filtrado logico\n",
    "df_drop_cl[df_drop_cl.HDI>0.6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El acceso a las filas de `df_drop_cl` se hace usando la notación de `Numpy`, cabe señalar que el acceso a los elementos del DataFrame no dependen de la indexación, que en este caso son países y no números enteros. Por otra parte, el operador `slice` puede operar en los índices no numéricos e \"incluye los extremos\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:21.626913Z",
     "start_time": "2018-07-25T02:15:21.612467Z"
    }
   },
   "outputs": [],
   "source": [
    "# A diferencia de Pyhon (sin Pandas), se incluyen los valores extremos\n",
    "df_drop_cl['Argentina':'Bolivia']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "* Reemplace en `df_drop_cl` los valores de `HDI` mayores a `0.7` por `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una herramienta bastante usada es el filtrado booleano en DataFrames. Su sintaxis es aquella ya introducida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:22.848609Z",
     "start_time": "2018-07-25T02:15:22.837500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Revisa que valores de dla columna HDI son mayores que 0.7\n",
    "print('Comparacion por columna: \\n',df_drop_cl.HDI >0.7,'\\n')\n",
    "\n",
    "print('Comparacion por columnas numericas: \\n',df_drop_cl.drop('Reg', axis=1) > 1,'\\n')\n",
    "\n",
    "# Filtrado \n",
    "print('Filtrado usando comparacion de una columna: \\n',df_drop_cl[df_drop_cl.HDI >0.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección con `loc` e `iloc`\n",
    "\n",
    "Hasta ahora, se han manipulado los índices y las columnas de DataFrames. En este contexto, las filas de un DataFrame se han manipulado por medio de `reindexing` y por métodos de llenado `ffill`. Por otra parte, para acceder a columnas (vistas como series), se tiene acceso a comandos  como los métodos `DataFrame.columna` y `Dataframe['columna']`. Los comandos análogos son `loc` e `iloc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:23.530253Z",
     "start_time": "2018-07-25T02:15:23.510058Z"
    }
   },
   "outputs": [],
   "source": [
    "# se selecciona la fila cuyo indice es 'Bolivia'\n",
    "print(DF.loc['Bolivia'],'\\n')\n",
    "\n",
    "# Se seleccionan las filas de Bolivia y Peru OBS. [[]]\n",
    "print(DF.loc[['Bolivia','Peru']],'\\n')\n",
    "\n",
    "# Mismo ejericios usando iloc.\n",
    "print(DF.iloc[1],'\\n')\n",
    "print(DF.iloc[[1,2]],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreciar, `loc` trabaja seleccionando el nombre especifico del índice que se desea obtener, mientras `iloc` lo hace con su posición numérica absoluta en el DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "* Combine la selección de columnas y filas para seleccionar de `DF` aquellos países entre  **Bolivia** y **Peru** que posean un **HDI** mayor que `0.75`. Use solo una linea de código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones aritméticas\n",
    "\n",
    "Como fue visto, Python no puede manejar sumas entre listas de manera nativa. En el caso de Series y DataFrames, `Pandas` permite operaciones aritméticas.\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "* Defina dos series de distinta longitud, sume ambas series. ¿Qué sucede con los valores fuera del rango para la serie más pequeña?. Repita el ejercicio para índices del tipo 'a','b', c'...\n",
    "\n",
    "De la misma manera, se pueden aplicar estas operaciones a DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:25.346505Z",
     "start_time": "2018-07-25T02:15:25.335812Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se define el dataframe DF2 para los paises en c2\n",
    "c2 = ['Ethiopia','Mali','Madagascar']\n",
    "\n",
    "DF2 = df[df.Country.isin(c2)]\n",
    "\n",
    "DF2.index = DF2.Country\n",
    "DF2 = DF2.drop(['Country','Reg'], axis=1)\n",
    "# DF2.drop(['Country','Reg'], inplace = True, axis=1) funciona de la misma forma.\n",
    "\n",
    "DF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "* Remueva las columnas `Country` y `Reg` de `DF`. ¿Qué resultado se espera de la operación `DF` + `DF2`?, ¿como se compara con el método `append`? \n",
    "\n",
    "* Use `np.arange`(similar a `range`) para generar dos DataFrames: `df1` con `3` filas y `4` columnas y `df2` con 4 filas y  5 columnas, ambos con índices numéricos y columnas del tipo `a,b,c...`. Sume ambos DataFrames y llene los valores faltantes con `0`.(hint: `list('abcd')` es equivalente a `['a','b','c','d']`, use el método `add`)\n",
    "\n",
    "Las operaciones aritméticas sobre un DataFrame se hacen en cada elemento de este, puede comprobar esto haciendo `1/df1`, `8*df1`, `df1**2` por ejemplo. ¿En que se diferencian estas operaciones con `DF` + `DF2`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones entre DataFrames y Series\n",
    "\n",
    "El resultado de resta entre un DataFrame y una serie puede no ser el esperado, en estos objetos la indexación juega un papel fundamental:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:26.894094Z",
     "start_time": "2018-07-25T02:15:26.861590Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ej = pd.DataFrame(np.arange(10).reshape(5, 2) +1,columns=list('ab'))\n",
    "df_ej2 = pd.DataFrame(np.arange(18).reshape(6, 3) +1000,columns=list('acb'))\n",
    "\n",
    "ser1 = np.repeat(1000,6)\n",
    "ser1 = pd.Series(ser1)\n",
    "\n",
    "print('df_ej  : \\n', df_ej)\n",
    "print()\n",
    "print('df_ej2 : \\n', df_ej2)\n",
    "print()\n",
    "# los indices de la serie \n",
    "print('Caso 1: Suma DataFrames')\n",
    "print(df_ej + df_ej2,'\\n')\n",
    "\n",
    "print('Caso 2: DataFrame + serie')\n",
    "print(df_ej + ser1,'\\n')\n",
    "\n",
    "# Se traspone el data frame antes de la suma\n",
    "print('Caso 3: DataFrame traspuesto + serie')\n",
    "print(df_ej.T + ser1)\n",
    "\n",
    "# Columnas de un DataFrame traspuesto\n",
    "\n",
    "print('\\n Columnas del DataFrame traspuesto: \\n', df_ej.T.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la celda anterior, se puede deducir que la suma de series y DataFrames se comporta como una \"unión externa\" basada en las columnas e índices: \n",
    "\n",
    "* En el caso 1, los DataFrames comparten las columnas `a` y `b` donde los valores se suman uno a uno, se añade la columna `c` (unión externa) con los valores faltantes correspondientes.\n",
    "\n",
    "* En el caso 2, la serie se comporta como un arreglo de `NumPy` en donde se suma de manera \"horizontal\". (Por tanto los índices pasan a ser columnas, trasponer la serie no cambia este hecho)\n",
    "\n",
    "* En el caso 3, las columnas del DataFrame pasan a ser las nuevas columnas, por lo que la suma se comporta como seria de esperar.\n",
    "\n",
    "Si a diferencia del caso 2, se desea sumar a través de las columnas, es posible hacerlo indicando el eje `axis`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:27.686723Z",
     "start_time": "2018-07-25T02:15:27.674624Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ej.add(ser1,axis=0).fillna(value=':)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicación de funciones\n",
    "\n",
    "Como ya se mencionó, los objetos de Pandas se comportan de manera similar a arreglos de `NumPy`. En este aspecto, los DataFrames y Series de Pandas, soportan de manera nativa las funciones de *elemento por elemento* :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output escalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:28.981107Z",
     "start_time": "2018-07-25T02:15:28.974525Z"
    }
   },
   "outputs": [],
   "source": [
    "df_c = pd.DataFrame(np.random.randn(5, 3), columns=['Arica','Santiago','Temuco'])\n",
    "\n",
    "metrica_1 = lambda x: (np.sum(x, axis = 0)**2)/len(x)\n",
    "metrica_2 = lambda x: np.abs(x).max()\n",
    "\n",
    "# Aplicación \"Clasica\" de la función metrica_1\n",
    "print(metrica_1(df_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output multidimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:29.822840Z",
     "start_time": "2018-07-25T02:15:29.810796Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def resumen_1(df):\n",
    "    return(pd.Series([metrica_1(df),metrica_2(df)]))\n",
    "\n",
    "df_c.apply(resumen_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones elemento por elemento\n",
    "En DataFrames usa el método `applymap` , en series `map`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:30.755087Z",
     "start_time": "2018-07-25T02:15:30.741676Z"
    }
   },
   "outputs": [],
   "source": [
    "# Identico al caso anterior pero usando el método apply\n",
    "\n",
    "print('Método clasico \"apply\":')\n",
    "print(df_c.apply(metrica_2))\n",
    "print()\n",
    "print('Elemento por elemento \"map\":')\n",
    "print(df_c.applymap(metrica_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "* Declare una función que calcule el promedio de valores `SCI` el DataFrame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordenamiento e índices duplicados\n",
    "\n",
    "Pandas permite el ordenamiento lexicográfico por filas (índices) o columnas a través del método `sort_index`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:31.940724Z",
     "start_time": "2018-07-25T02:15:31.933648Z"
    }
   },
   "outputs": [],
   "source": [
    "obj = pd.Series(np.random.randn(4), index=['d', 'a', 'b', 'c'])\n",
    "\n",
    "print(obj.sort_index())\n",
    "print()\n",
    "print(obj.sort_index(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T21:02:03.224358Z",
     "start_time": "2018-07-13T21:02:03.220923Z"
    }
   },
   "source": [
    "**Ejericicios**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El DataFrame `df` definido en esta [sección](#link), contiene los datos usados en la `Tarea 1`. Declare una función que calcule el promedio `SCI` y `HDI` de `df`.\n",
    "\n",
    "* Ordene de manera descendiente los valores de `df` según `HDI` y según país. (Hint: `by`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:33.160182Z",
     "start_time": "2018-07-25T02:15:33.137352Z"
    }
   },
   "outputs": [],
   "source": [
    "display(df.head(5))\n",
    "\n",
    "mean_sci = np.mean(df.SCI)\n",
    "mean_hdi = np.mean(df.HDI)\n",
    "\n",
    "df.sort_values(by='HDI', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al unificar bases de datos con distintos orígenes, es posible que aparezcan índices duplicados dentro de un DataFrame. Dentro del atributo `index` existe la propiedad `is_unique`, esta indica si los índices de un DataFrame son todos únicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:33.918114Z",
     "start_time": "2018-07-25T02:15:33.913894Z"
    }
   },
   "outputs": [],
   "source": [
    "obj = obj.reindex(['a','a','b','c'])\n",
    "obj.index.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `unique` extrae los valores únicos dentro en una serie, mientras que en el caso de DataFrames, es necesario el uso del método `apply` para aplicar conteo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:34.970772Z",
     "start_time": "2018-07-25T02:15:34.966233Z"
    }
   },
   "outputs": [],
   "source": [
    "obj.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estadística descriptiva básica<a id=\"estaDesc\"></a>\n",
    "\n",
    "Los objetos de `Pandas` están provistos de métodos estadísticos básicos.\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "* Explore los métodos `sum`, `cumsum` `mean`, `std`, `idxmin`, `idxmax` y `describe` (especialmente útil)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza y preparación de datos<a id=\"limPrep\"></a>\n",
    "\n",
    "Gran parte del proceso de ciencia de datos se invierte en la preparación de los datos, esto comprende la carga, limpieza, transformación y reorganización. Es posible *modularizar* los procesos y manejar distintos procesos en distintos lenguajes (como R o Python). Si se desea implementar procesos de limpieza y transformación de datos en Python, esto se puede hacer usando `Pandas`. En esta sección se profundiza en el manejo de datos faltantes, filtrado, llenado y transformación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos faltantes\n",
    "\n",
    "La falta de información en bases de datos es común en tareas de análisis de datos, la filosofía de `Pandas` en es este aspecto se basa en facilitar el manejo de datos faltantes lo más posible. En el caso de  datos numéricos, `Pandas` usa la notación `NaN` (not a number) para representar valores faltantes, este valor se denota como *valor centinela*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:36.510812Z",
     "start_time": "2018-07-25T02:15:36.502494Z"
    }
   },
   "outputs": [],
   "source": [
    "string_data = pd.Series(['S_1','S_2',np.nan,'S_4'])\n",
    "string_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:37.232788Z",
     "start_time": "2018-07-25T02:15:37.227697Z"
    }
   },
   "outputs": [],
   "source": [
    "string_data.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En `R` la convención consiste en denotar los valores faltantes como `NA` (not avaible). En estadística, esta notación aparece cuando no existe la información buscada o existe pero no fue ingresada. Al limpiar los datos es por tanto importante estudiar la estructura de los datos faltantes como un tema de estudio por si solo. En Python el valor `None` juega el papel de `NA` en `R`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:38.161891Z",
     "start_time": "2018-07-25T02:15:38.157586Z"
    }
   },
   "outputs": [],
   "source": [
    "string_data[0] = None\n",
    "string_data.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "* Estudie los métodos `dropna`, `fillna`, `isnull` y `notnull`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrado de valores faltantes\n",
    "\n",
    "Existen varias maneras de filtrar datos faltantes. Una forma es hacerlo usando filtrado booleano a través del método `isnull`, otra más directa es usar el método `dropna`. En series por ejemplo, este último método retorna una sub-serie consistente solo de los valores presentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:39.271570Z",
     "start_time": "2018-07-25T02:15:39.268578Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import nan as NA\n",
    "data = pd.Series([2, NA, 4, NA, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:39.850773Z",
     "start_time": "2018-07-25T02:15:39.844635Z"
    }
   },
   "outputs": [],
   "source": [
    "print(data.dropna()) #dropna --> método directo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:40.244390Z",
     "start_time": "2018-07-25T02:15:40.239451Z"
    }
   },
   "outputs": [],
   "source": [
    "print(data[data.notnull()]) # filtrado logico ---> indirecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de DataFrames el proceso cambia. El aumento en la dimensión provoca la complicación y básicamente depende si se desean borrar filas o columnas, y en caso de querer eliminar es necesario tener claro como se desea hacer este proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:41.329979Z",
     "start_time": "2018-07-25T02:15:41.316725Z"
    }
   },
   "outputs": [],
   "source": [
    "data_frame = pd.DataFrame([[1,2,4], [8,NA,NA],\n",
    "                         [NA,NA,NA], [NA,NA,16],\n",
    "                          [32,NA,64]])\n",
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:41.763714Z",
     "start_time": "2018-07-25T02:15:41.752167Z"
    }
   },
   "outputs": [],
   "source": [
    "limpio_1 = data_frame.dropna()\n",
    "limpio_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando `how = 'all'` eliminará solo aquellas filas consistentes solo de valores faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:42.638077Z",
     "start_time": "2018-07-25T02:15:42.626616Z"
    }
   },
   "outputs": [],
   "source": [
    "limpio_2 = data_frame.dropna(how='all')\n",
    "limpio_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "* En elimine de la variable `data_frame` aquellas columnas consistentes únicamente de valores faltantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llenado de datos faltantes\n",
    "\n",
    "Otra manera de atacar el problema es completando ciertos valores faltantes en vez de filtrarlos. En `Pandas` se puede usar el método `fillna`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Ejercicio**\n",
    "\n",
    "* Genere un DataFrame aleatorio de `10x10` consistente de números aleatorios. Llene los primeros `5` elementos de la primera columna con `NA`. Haga lo mismo con los 3 primeros elementos de la segunda fila. (Hint: `iloc` + `randn` )\n",
    "\n",
    "* Investigue el argumento `method = ffill` del método `fill_na`.\n",
    "\n",
    "* Seleccione la tercera fila del DataFrame y reemplace los valores faltantes con el promedio de los demás valores. (Hint: obtenga los índices no faltantes usando `~` + `isnull`, calcule el promedio y reemplace)\n",
    "\n",
    "* Reemplace los valores faltantes de la primera columna con el valor 10, los de la segunda columna con el valor 100 y los de la tercera columna con el el valor 1000. Para ello use la función `fillna` y proporcione un diccionario como input del método."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación de datos\n",
    "\n",
    "En esta sección se discuten las operaciones de transformación que suceden a la limpieza y reorganización de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:45.001032Z",
     "start_time": "2018-07-25T02:15:44.989512Z"
    }
   },
   "outputs": [],
   "source": [
    "data_frame_dup = pd.DataFrame({'A1':['uno', 'dos'] * 3 + ['dos'],\n",
    "                               'A2':[1,1,2,3,3,4,4]})\n",
    "data_frame_dup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "El método `duplicated` proporciona un serie booleana indicando que serie esta duplicada (observada anteriormente). Por su parte, `drop_duplicates` corresponde al símil de `drop_na` para `isnull`. \n",
    "\n",
    "* Utilice tales métodos en la serie anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación de datos por mapping\n",
    "\n",
    "El *mapping* o aplicación, corresponde a la trasformación de valores en una serie basándose en los valores que esta contiene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:46.142537Z",
     "start_time": "2018-07-25T02:15:46.133602Z"
    }
   },
   "outputs": [],
   "source": [
    "data_frame_mapping = pd.DataFrame({'receta':['huevos', 'harina', 'leche','manjar'],\n",
    "                                  'cantidad':[2,1,1,250]})\n",
    "data_frame_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se agrega una columna basada en los valores del DataFrame (mapping):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:46.781962Z",
     "start_time": "2018-07-25T02:15:46.779582Z"
    }
   },
   "outputs": [],
   "source": [
    "es_vegano = {'huevos':'no', 'harina':'si', 'leche':'no','manjar':'no'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `map` acepta un objeto del tipo `dict` que contiene una relación para los elementos de la serie que opera.\n",
    "\n",
    "Observación: Hacer `mapping` es *case sensitive*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:47.564226Z",
     "start_time": "2018-07-25T02:15:47.552556Z"
    }
   },
   "outputs": [],
   "source": [
    "data_frame_mapping['es vegano ?'] = data_frame_mapping['receta'].map(es_vegano)\n",
    "data_frame_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, `map` puede recibir una función como elemento y la opera sobre cada elemento de la serie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:48.334393Z",
     "start_time": "2018-07-25T02:15:48.329359Z"
    }
   },
   "outputs": [],
   "source": [
    "data_frame_mapping['es vegano ?'].map(lambda x: x.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un caso especial de remplazo/mapping es el método `replace` este es más general que `fillna` pues permite remplazar valores a elección de una serie.\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "* Reemplace en la siguiente serie los valores 'error' y 'Error' por NA y 100 respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:49.060986Z",
     "start_time": "2018-07-25T02:15:49.053643Z"
    }
   },
   "outputs": [],
   "source": [
    "serie = pd.Series([1, 'error', 2, 'error', -1000., 3, 'Error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretización y binning\n",
    "\n",
    "Los datos numéricos *continuos* pueden ser separados en secciones o *bins* para su estudio. Como ejemplo se considera la siguiente serie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:49.559317Z",
     "start_time": "2018-07-25T02:15:49.554696Z"
    }
   },
   "outputs": [],
   "source": [
    "income = [98,2, 28,11,53,61,33,17,19,40,78,8,3,13,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se desea dividir tal serie en bins de 0 a 30, 31 a 50, 51 a 70, 71 a 90  y 91 al 'infinito'. Para ello se usa la función `cut`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:50.130990Z",
     "start_time": "2018-07-25T02:15:50.121105Z"
    }
   },
   "outputs": [],
   "source": [
    "bins = [30,50,70,90]\n",
    "secciones = pd.cut(income,bins)\n",
    "secciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:50.442648Z",
     "start_time": "2018-07-25T02:15:50.438153Z"
    }
   },
   "outputs": [],
   "source": [
    "secciones.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:50.748759Z",
     "start_time": "2018-07-25T02:15:50.741664Z"
    }
   },
   "outputs": [],
   "source": [
    "secciones.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:51.080043Z",
     "start_time": "2018-07-25T02:15:51.073169Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.value_counts(secciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detección y filtrado de outliers\n",
    "\n",
    "La detección y manejo de outliers corresponde a la detección de valores fuera de rango a través de operaciones en arreglos. \n",
    "\n",
    "**Ejercicios**\n",
    "\n",
    "* Genere un DataFrame de 4 columnas con 1000 observaciones. Utilice `describe` para estudiar las características de su DataFrame.\n",
    "\n",
    "* Seleccione aquellos valores del dataframe mayores que 1.5 y menores que -1,5 y reemplace esos valores por 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutaciones y muestreo aleatorio\n",
    "\n",
    "Permutar corresponde a reodernar los índices de una serie o las filas de un DataFrame, en aplicaciones de análisis de datos se requiere hacer permutaciones aleatorias en los datos. Esto se puede hacer fácilmente usando la función `permutation` del módulo `random`, donde tal función se debe llamar usando como argumento la longitud del eje `axis` sobre el cual se desea permutar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:51.874691Z",
     "start_time": "2018-07-25T02:15:51.857122Z"
    }
   },
   "outputs": [],
   "source": [
    "data_frame_perm = pd.DataFrame(np.arange(10*10).reshape((10,10)))\n",
    "data_frame_perm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:52.214373Z",
     "start_time": "2018-07-25T02:15:52.211006Z"
    }
   },
   "outputs": [],
   "source": [
    "sampler = np.random.permutation(10)\n",
    "sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `take` proporciona por defecto las filas del DataFrame tomadas de un arreglo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:52.618788Z",
     "start_time": "2018-07-25T02:15:52.607666Z"
    }
   },
   "outputs": [],
   "source": [
    "data_frame_perm.take(sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para seleccionar un subconjunto sin reemplazo, se puede usar directamente el método `sample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:53.119933Z",
     "start_time": "2018-07-25T02:15:53.110035Z"
    }
   },
   "outputs": [],
   "source": [
    "data_frame_perm.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observación: Para seleccionar con reemplazo existe la opción `replace = True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables  indicadores\n",
    "\n",
    "Otro tipo de transformación es el manejo de variables `Dummy`o indicadores. Estas consisten en 'códigos' que representan a través de números los distintos valores que puede tomar una variable categórica de interés.\n",
    "\n",
    "Si un DataFrame tiene `k` distintos valores en una columna, se puede obtener una matriz indicadora con `k` columnas consistentes de 1's y 0's y usar aquellos valores como variables dummy para representar tal columna.\n",
    "\n",
    "En pandas la función `get_dummies` genera tal matriz indicadora de manera sencilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:54.875804Z",
     "start_time": "2018-07-25T02:15:54.865665Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dummy = pd.DataFrame({'llave_1':['a','a','b','c','b','a'],\n",
    "                            'data_1':range(6)})\n",
    "\n",
    "data_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:55.233287Z",
     "start_time": "2018-07-25T02:15:55.217022Z"
    }
   },
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(data_dummy['llave_1'])\n",
    "dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para generar un nuevo DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:15:56.075246Z",
     "start_time": "2018-07-25T02:15:56.061625Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dummy_mod = data_dummy[['data_1','llave_1']].join(dummies)\n",
    "data_dummy_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga y guardado de datos<a id=\"carga\"></a>\n",
    "\n",
    "En aplicaciones reales, es necesario importar y exportar datos en múltiples formatos, `Pandas` permite el manejo de archivos `csv`,`excel`,`html`,`json`,`sql`, entre otros.\n",
    "\n",
    "En estos tipo de archivos, `Pandas` infiere de manera automática la indexación, formatos de fecha, permite iteración sobre elementos y maneja problemas relativos a fallas de limpieza en los datos, tales como errores de escritura o formatos numéricos no estándar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios**\n",
    "\n",
    "* Cargue los datos `export_part.xls`  de la carpeta `datos/C2`. Cargue la planilla `data` del documento. \n",
    "\n",
    "Observación: En ese dataset la columna de indexación es `productos principales`.\n",
    "\n",
    "* Cambie todos los valores `...` por `NaN` o un valor de su elección.\n",
    "\n",
    "* Cambie los nombres de las columnas de formato `int` a `str`.\n",
    "\n",
    "* Guarde las datos trasformados en formato `csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:16:02.016312Z",
     "start_time": "2018-07-25T02:16:01.995438Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel('../cepal_estudiantes/datos/C2/export_part.xls',sheet_name='data', index_col=0)\n",
    "data.replace('...','0',inplace=True)\n",
    "\n",
    "data.head()\n",
    "data.columns = [str(data.columns[i]) for i in range(len(data.columns))]\n",
    "data['2007']\n",
    "data.to_csv('../data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos en formato JSON\n",
    "\n",
    "JSON (short JavaScript Objet Notation) es un formato estándar de envío de datos por HTTP entre navegadores web y otras aplicaciones. Su estructura es más flexible que los formatos delimitados (CSV por ejemplo). El formato JSON se comporta de manera similar a los diccionarios de Python. Para el manejo de este formato existen distintas librerías, entre ellas `json`, que se encuentra dentro de las librerías estándar de Python. A continuación se estudia este formato y ciertas operaciones disponibles para su manejo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un archivo JSON corresponde a un `string`, este se formatea siguiendo el esquema `\"\"\"{campo_1 : dato_1, ,campo_n : dato_n}\"\"\"`, un ejemplo se puede observar en la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:14:39.522353Z",
     "start_time": "2018-07-25T02:14:39.515564Z"
    }
   },
   "outputs": [],
   "source": [
    "json_ej = \"\"\"\n",
    "{\"type\":\"FeatureCollection\",\n",
    " \"generated\":5,\n",
    " \"url\":\"www.ejemplo.cl\",\n",
    " \"title\":\"titulo ejemplo\",\n",
    " \"api\":\"super api\",\n",
    " \"count\": 8,\n",
    " \"status\": 1,\n",
    " \"geometry\":{\"type\":\"Point\",\"coordinates\":[1,2,10]},\n",
    " \"id\":\"19101818712-k\"\n",
    " }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, el formato anterior responde a una estructura similar a la de los diccionarios. El módulo json es capas de convertir la variable `json_ej` en un diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:14:41.108200Z",
     "start_time": "2018-07-25T02:14:41.102146Z"
    }
   },
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "# json_loads permite cargar un archivo json a un diccionario python\n",
    "\n",
    "res = json.loads(json_ej)\n",
    "print(res.keys(), '\\n')\n",
    "\n",
    "# La función json.dumps lo hace en sentido contrario:\n",
    "json_ej2 = json.dumps(res)\n",
    "print(json_ej2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente, se obtiene una lista objetos JSON, estos se convierten a diccionarios de Python y se convierten a DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manejo de API's <a id=\"API\"></a>\n",
    "\n",
    "Una API (interfaz de programación de aplicaciones) es un conjunto de métodos que ofrece ciertas funcionalidades para ser utilizadas por otro software. Muchas paginas web tienen API's publicas que proveen datos por medio de JSON's u otro formato. Existen muchas maneras de acceder a una API; con el fin de explorar las funciones relativas al manejo del formato JSON y obtener herramientas para interactuar con API's, se estudian un ejemplo y un ejercicio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T22:19:57.997611Z",
     "start_time": "2018-07-13T22:19:57.994317Z"
    }
   },
   "source": [
    "**Ejemplo 1: Manejo de API USGS**\n",
    "\n",
    "Anteriormente, se trabajo con archivos JSON sobre la base de terremotos patrocinada por el programa de amenazas por terremoto de la USGS (Earthquake Hazards Program). En este ejemplo, se utilizará la documentación de su API para estudiar los movimientos sísmicos ocurridos en el centro-sur de Chile desde el 2009. En este caso se hará uso de la librería `requests` de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:52.258108Z",
     "start_time": "2018-07-25T02:10:50.568249Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fork del codigo creado por ayanez3\n",
    "se añade busqueda usando API + JSON + Pandas\n",
    "\n",
    "Aplica filtro, mapeo  y reduccion de datos\n",
    "https://earthquake.usgs.gov/\n",
    "https://earthquake.usgs.gov/fdsnws/event/1/\n",
    "https://earthquake.usgs.gov/earthquakes/feed/v1.0/geojson.php\n",
    "\n",
    "Transforma a dataframe con reconocimiento automatico de campos JSON,\n",
    "este se puede almacenar en el formato que se estime conveniente.\n",
    "\"\"\"\n",
    "# https://www.coordenadas-gps.com/ coordenadas\n",
    "\n",
    "# maxlatitude -40\n",
    "# minlatitude -74 \n",
    "# maxlongitude-31\n",
    "# minlongitude-70\n",
    "# minmagnitude 5 \n",
    "\n",
    "# API earthquakes\n",
    "# https://earthquake.usgs.gov/fdsnws/event/1/\n",
    "\n",
    "# Consulta en aplicacion web\n",
    "# https://earthquake.usgs.gov/earthquakes/map/#%7B%22feed%22%3A%221531517750969%22%2C%22sort%22%3A%22newest%22%2C%22mapposition%22%3A%5B%5B-40.413%2C-74.18%5D%2C%5B-32.094%2C-69.653%5D%5D%2C%22viewModes%22%3A%5B%22list%22%2C%22map%22%5D%2C%22autoUpdate%22%3Afalse%2C%22search%22%3A%7B%22id%22%3A%221531517750969%22%2C%22name%22%3A%22Search%20Results%22%2C%22isSearch%22%3Atrue%2C%22params%22%3A%7B%22starttime%22%3A%222009-07-06%2000%3A00%3A00%22%2C%22endtime%22%3A%222018-07-13%2023%3A59%3A59%22%2C%22maxlatitude%22%3A-32.094%2C%22minlatitude%22%3A-40.413%2C%22maxlongitude%22%3A-69.653%2C%22minlongitude%22%3A-74.18%2C%22minmagnitude%22%3A5%2C%22orderby%22%3A%22time%22%7D%7D%7D\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "query = {\n",
    "         'starttime'   :'2009-01-01', \n",
    "         'minlatitude' :-40,\n",
    "         'minlongitude':-74,\n",
    "         'maxlatitude' :-31,\n",
    "         'maxlongitude':-70,\n",
    "         'minmagnitude': 5\n",
    "        }\n",
    "\n",
    "# Compresion de listas para generar una consulta\n",
    "q = ['&'+str(key)+'='+str(query[str(key)]) for key in query.keys()]\n",
    "\n",
    "###\n",
    "\n",
    "# El metodo join de str (str.join) funciona de la siguiente forma:\n",
    "#\n",
    "# s = \"***\" \n",
    "# seq = [\"a\", \"b\", \"c\"]\n",
    "# print(s.join(seq)) ===> a***b***c\n",
    "\n",
    "###\n",
    "\n",
    "q = ''.join(q)\n",
    "\n",
    "url_base = \"https://earthquake.usgs.gov/fdsnws/event/1/query?\"\n",
    "url_format = \"geojson\"\n",
    "\n",
    "url = url_base +'format=' + url_format + q\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "records = data[\"features\"] # genera lista de diccionarios\n",
    "\n",
    "print(\"listo!!!, hay {} registros\".format(len(records)))\n",
    "\n",
    "def map00(x):return x[\"properties\"]\n",
    "\n",
    "def map01(x): return {\n",
    "        \"place\": x[\"place\"][:29]+'...', \n",
    "        \"timestamp\": dt.datetime.fromtimestamp(x[\"time\"]//1000),\n",
    "        \"mag\": x[\"mag\"] } \n",
    "\n",
    "def filtra_0(x,m): return x[\"mag\"] >= m\n",
    "\n",
    "def ordena(a): return a[\"mag\"]\n",
    "\n",
    "# Evaluacion parcial de filtra_0 \n",
    "filtra = lambda x: filtra_0(x,6)\n",
    "\n",
    "t = list(map(map00, records))    # primer mapeo para simplificar estructura \n",
    "u = list(map(map01, t))          # segundo mapeo para reducir numero de campos y aplicar algunas transformaciones \n",
    "v = list(filter(filtra, u))      # filtra por magnitud\n",
    "v.sort(key=ordena, reverse=True) # ordena  por magnitud\n",
    "\n",
    "df_usgs = pd.DataFrame(v)\n",
    "\n",
    "display(df_usgs.head(10))\n",
    "print(\"Estadísticos sobre la magnitud:\")\n",
    "df_usgs.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "A continuación interactuará con la API de twitter usando Twython: \n",
    "\n",
    "```\n",
    "pip install twtython\n",
    "```\n",
    "\n",
    "Se debe tener en cuenta que este paquete es solo un wrapper y que no es el único que esta diseñado para comunicarse con twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:52.504104Z",
     "start_time": "2018-07-25T02:10:52.475077Z"
    }
   },
   "outputs": [],
   "source": [
    "from twython import Twython  \n",
    "\n",
    "# Enter your keys/secrets as strings in the following fields\n",
    "credentials = {}  \n",
    "credentials['CONSUMER_KEY']=\"\"\n",
    "credentials['CONSUMER_SECRET']=\"\"\n",
    "credentials['ACCESS_TOKEN']=\"\"  \n",
    "credentials['ACCESS_SECRET']=\"\"\n",
    "\n",
    "# Save the credentials object to file\n",
    "with open(\"twitter_credentials.json\", \"w\") as file:  \n",
    "    json.dump(credentials, file)\n",
    "\n",
    "# Load credentials from json file\n",
    "with open(\"twitter_credentials.json\", \"r\") as file:  \n",
    "    creds = json.load(file)\n",
    "    \n",
    "python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:53.398681Z",
     "start_time": "2018-07-25T02:10:53.395716Z"
    }
   },
   "outputs": [],
   "source": [
    "query = {'q': 'machine learning',  \n",
    "        'result_type': 'popular',\n",
    "        'count': 10,\n",
    "        'lang': 'en',\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:54.322380Z",
     "start_time": "2018-07-25T02:10:53.708229Z"
    }
   },
   "outputs": [],
   "source": [
    "H = python_tweets.search(**query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:54.325546Z",
     "start_time": "2018-07-25T02:10:53.887Z"
    }
   },
   "outputs": [],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T16:17:35.603941Z",
     "start_time": "2018-07-17T16:17:35.596790Z"
    }
   },
   "source": [
    "Búsqueda de tweets, se almacena el usuario, fecha, texto y conteo de favoritos (opcional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:57.219598Z",
     "start_time": "2018-07-25T02:10:56.977476Z"
    }
   },
   "outputs": [],
   "source": [
    "dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  \n",
    "for status in python_tweets.search(**query)['statuses']:  \n",
    "    dict_['user'].append(status['user']['screen_name'])\n",
    "    dict_['date'].append(status['created_at'])\n",
    "    dict_['text'].append(status['text'])\n",
    "    dict_['favorite_count'].append(status['favorite_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se estructura en un DataFrame de pandas para su manipulación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:57.682362Z",
     "start_time": "2018-07-25T02:10:57.674791Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict_)  \n",
    "df.sort_values(by='favorite_count', inplace=True, ascending=False)  \n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la pregunta anterior se le solicitó que hiciera una búsqueda única, para hacer una búsqueda continua o una colección `stream` de tweets se puede hacer uso de la API de streaming de twitter. Para ello se hace uso de la clase `TwythonStreamer` para ello, se creará un objeto que heredará los atributos y métodos de tal clase, en dicho objeto, se creará una acción para consultas exitosas `on_success` y consultas fallidas `on_error`.\n",
    "\n",
    "En este caso se almacenan los hashtags, nombres de usuario, ubicación del usuario y el texto del tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importan las librerías necesarias y se seleccionan los datos de interés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:58.350645Z",
     "start_time": "2018-07-25T02:10:58.345676Z"
    }
   },
   "outputs": [],
   "source": [
    "from twython import TwythonStreamer  \n",
    "import csv\n",
    "\n",
    "def process_tweet(tweet):  \n",
    "    d = {}\n",
    "    d['hashtags'] = [hashtag['text'] for hashtag in tweet['entities']['hashtags']]\n",
    "    d['text'] = tweet['text']\n",
    "    d['user'] = tweet['user']['screen_name']\n",
    "    d['user_loc'] = tweet['user']['location']\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea la clase `MyStreamer` que hereda de la calse `TwythonStreamer`, el método `on_succes` guardan la información obtenida a un csv mientras que `on_error` desconecta en caso de error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:10:58.678819Z",
     "start_time": "2018-07-25T02:10:58.674804Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a class that inherits TwythonStreamer\n",
    "lang = 'en'\n",
    "\n",
    "class MyStreamer(TwythonStreamer):     \n",
    "    \n",
    "    # Datos obtenidos\n",
    "    def on_success(self, data):\n",
    "\n",
    "        # Obtiene tweets en idioma \"lang\"\n",
    "        if data['lang'] == lang:\n",
    "            tweet_data = process_tweet(data)\n",
    "            self.save_to_csv(tweet_data)\n",
    "                    \n",
    "    # En caso de presentarse problemas con la API desconecta\n",
    "    def on_error(self, status_code, data):\n",
    "        print(status_code, data)\n",
    "        self.disconnect()\n",
    "\n",
    "    # Guarda los tweets en un csv\n",
    "    def save_to_csv(self, tweet):\n",
    "        with open(r'saved_tweets.csv', 'a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(list(tweet.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un instancia de la clase anterior usando las credenciales de ingreso como argumentos, el método `filter` permitirá colectar los tweets de interés.\n",
    "\n",
    "Es posible mejorar la búsqueda agregado parámetros como idioma, ubicaciones, usuarios seleccionados, etc ...\n",
    "\n",
    "Observación: La versión de paga incluye más opciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:11:00.490235Z",
     "start_time": "2018-07-25T02:10:59.068724Z"
    }
   },
   "outputs": [],
   "source": [
    "stream = MyStreamer(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'],  \n",
    "                    creds['ACCESS_TOKEN'], creds['ACCESS_SECRET'])\n",
    "\n",
    "# Proceso de streaming\n",
    "stream.statuses.filter(track='machine learning') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:11:00.727072Z",
     "start_time": "2018-07-25T02:11:00.696863Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"saved_tweets.csv\", header= None)  \n",
    "tweets.columns=['hastags', 'text','user','location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:11:00.730402Z",
     "start_time": "2018-07-25T02:10:59.469Z"
    }
   },
   "outputs": [],
   "source": [
    "h = tweets.hastags.str.strip('[')\n",
    "h = h.str.strip(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T02:11:00.733956Z",
     "start_time": "2018-07-25T02:10:59.647Z"
    }
   },
   "outputs": [],
   "source": [
    "h = h.str.split().tolist()\n",
    "stacked = pd.DataFrame(h).stack()\n",
    "stacked.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 123.18181799999999,
   "position": {
    "height": "40px",
    "left": "1447.8px",
    "right": "20px",
    "top": "129px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
